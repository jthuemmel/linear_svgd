{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import my_funcs as my"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "cpu = torch.device('cpu')\n",
    "gpu = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def banana(x1,x2):\n",
    "    return torch.exp(-0.5*(0.01*x1**2+ 0.1*(x2+ 0.1*x1**2 -10)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate targets in a number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ds = [1,2,10,100]\n",
    "Ns = [1+d for d in Ds]\n",
    "num_iters = [1000*d for d in Ds]\n",
    "eta = 1e-2\n",
    "\n",
    "num_runs = 10\n",
    "\n",
    "\n",
    "names = ['linear svgd','rbf svgd','gpf']\n",
    "\n",
    "data_path = os.path.join(os.getcwd(), 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in range(num_runs):\n",
    "    \n",
    "    for i,D in enumerate(Ds):\n",
    "        #setup a standard normal distribution for the given dimension\n",
    "        standard_normal = torch.distributions.MultivariateNormal(torch.zeros(D),torch.eye(D))\n",
    "\n",
    "        #generate a random multivariate gaussian target\n",
    "        target_mean = standard_normal.sample()\n",
    "        target_covariance = my.random_covariance(D)\n",
    "        p = torch.distributions.MultivariateNormal(target_mean,target_covariance)\n",
    "\n",
    "        #sample initial particle positions x0\n",
    "        x0 = standard_normal.sample([Ns[i]])\n",
    "    \n",
    "        #run each algorithm\n",
    "    \n",
    "        x_gpf = my.gpf(x0,p,num_iter = num_iters[i],eta1 = eta, eta2 = eta, history = False)\n",
    "        x_lin = my.svgd(x0,p,num_iter = num_iters[i],kernel = 'linear',eps = eta, history = False)\n",
    "        x_rbf = my.svgd(x0,p,num_iter = num_iters[i],kernel = 'rbf',eps = eta, history = False)\n",
    "        \n",
    "        #\n",
    "        \n",
    "        run_name = os.path.join(data_path,'run_{}_dim_{}'.format(m+1,D))\n",
    "        if not os.path.exists(run_name):\n",
    "            os.mkdir(run_name)\n",
    "        \n",
    "        torch.save(p,os.path.join(run_name,'target'))\n",
    "        torch.save(x0,os.path.join(run_name,'x0'))\n",
    "        torch.save(x_gpf,os.path.join(run_name,'x_gpf'))\n",
    "        torch.save(x_lin,os.path.join(run_name,'x_lin'))\n",
    "        torch.save(x_rbf,os.path.join(run_name,'x_rbf'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## blub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate random covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "#generate diagonal matrix with eigenvalues in the desired range (i.e. [1e-1,1e1])\n",
    "B = torch.eye(D) * torch.distributions.Uniform(0.1,10).sample([D])\n",
    "#generate a random unitary matrix (use QR decomposition to obtain)\n",
    "Q,R = torch.qr(torch.rand([D,D]))\n",
    "#obtain the desired covariance by multiplication:\n",
    "sigma = torch.mm(torch.mm(Q,B),Q.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "tensor(True)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.allclose(Q,Q.T))\n",
    "print(torch.allclose(sigma,sigma.T))\n",
    "print(torch.det(sigma) > 0)\n",
    "print(torch.allclose(Q@Q.T,torch.eye(D),atol = torch.finfo(torch.float).eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_cov(D,Lmin=0.1,Lmax=10):\n",
    "    #generate diagonal matrix with eigenvalues in the desired range (i.e. [1e-1,1e1])\n",
    "    L = torch.eye(D) * torch.distributions.Uniform(Lmin,Lmax).sample([D])\n",
    "    #generate a random unitary matrix (use QR decomposition to obtain)\n",
    "    Q,R = torch.qr(torch.rand([D,D]))\n",
    "    #obtain the desired covariance by multiplication:\n",
    "    sigma = torch.mm(torch.mm(Q,L),Q.T)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a sample from the empirical distribution (in GPF, but potentially in svgd aswell?)\n",
    "def generate_sample(x):\n",
    "    N = x.shape[0]\n",
    "    D = x.shape[1]\n",
    "    M0 = torch.distributions.MultivariateNormal(torch.zeros(D),torch.eye(D))\n",
    "    m = torch.mean(x,0)\n",
    "    return(1/torch.sqrt(torch.as_tensor(N,dtype=float)))*torch.sum((x-m)*M0.sample([N]),0)+m\n",
    "\n",
    "#empirical covariance formula (could use numpy instead)\n",
    "def estimate_cov(x):\n",
    "    N = x.shape[0]\n",
    "    m = torch.mean(x,0)\n",
    "    assert N > 1\n",
    "    return torch.mm((x-m).T,(x-m))/(N)\n",
    "\n",
    "#efficient implementation of squared distance calculation (faster than cdist**2)\n",
    "def squared_distance(x):\n",
    "    norm = (x ** 2).sum(1).view(-1, 1)\n",
    "    dist_mat = (norm + norm.view(1, -1)) - 2.0 * torch.mm(x , x.t())\n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing GPF using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpf(x,num_iter=1000,eta1=0.1,eta2=0.1):\n",
    "    for j in range(num_iter):\n",
    "        #calculate score\n",
    "        x.requires_grad = True\n",
    "        log_px = -p.log_prob(x) \n",
    "        log_px.sum().backward(retain_graph = True)\n",
    "        #compute g and g_\n",
    "        g = x.grad\n",
    "        g_ = torch.mean(g,0)\n",
    "        #center the particles\n",
    "        x_m = x - torch.mean(x,0)\n",
    "        #calculate Matrix A particle-wise\n",
    "        a = torch.zeros([D,D])\n",
    "        for i in range(N):\n",
    "            a += torch.mm(g[i].view(D,1),x_m[i].view(1,D))\n",
    "        A = a/N - torch.eye(D)\n",
    "        #update particles\n",
    "        x_new = x - eta1*g_.view(1,D) - eta2*torch.mm(x_m,A) \n",
    "\n",
    "        x = x_new.clone().detach()\n",
    "    return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 3\n",
    "target_mean = torch.full([D],3,dtype=torch.float)\n",
    "#target_mean = torch.Tensor([-10.,-1.])\n",
    "target_covariance = torch.eye(D)*2\n",
    "#target_covariance = torch.Tensor([[5,2],[2,5]])\n",
    "\n",
    "#target distribution p(x)\n",
    "p = torch.distributions.MultivariateNormal(target_mean,target_covariance)\n",
    "#initial distribution q0(x)\n",
    "q0 = torch.distributions.MultivariateNormal(torch.zeros(D),torch.eye(D))\n",
    "#number of particles to sample from q0\n",
    "N = 4\n",
    "#number of iterations\n",
    "num_iter = 100\n",
    "#step size\n",
    "eta1 = 0.1\n",
    "eta2 = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = q0.sample([N])\n",
    "x = gpf(x0,num_iter,eta1,eta2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.mean(x,0)\n",
    "c = estimate_cov(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(m.allclose(target_mean,atol=1e-3))\n",
    "print(c.allclose(target_covariance,atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1ef98f32c40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(*x.T)\n",
    "plt.scatter(*m.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing svgd in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel(x,h=-1):\n",
    "    #number of particles\n",
    "    N = x.shape[0]\n",
    "    #number of dimensions\n",
    "    D = x.shape[1]\n",
    "    #pairwise squared euclidean distances\n",
    "    dist_mat = squared_distance(x)\n",
    "    #if no proper bandwidth is given\n",
    "    if h < 0:\n",
    "        #use the heuristic from liu&wang2016\n",
    "        h = torch.median(dist_mat)/torch.log(torch.FloatTensor([N+1]))\n",
    "    #rbf kernel formula\n",
    "    kxx = torch.exp(-dist_mat/h)\n",
    "    #vectorized form of the derivative of the rbf kernel\n",
    "    dxkxx = (-torch.mm(kxx,x)+kxx.sum(1).view(-1,1)*x)*(2/h)\n",
    "    return kxx, dxkxx\n",
    "\n",
    "\n",
    "def linear_kernel(x):\n",
    "    N = x.shape[0]\n",
    "    #D = x.shape[1]\n",
    "    kxx = torch.mm(x,x.T)+torch.ones([N,N])\n",
    "    dkxx = (N+1) * x\n",
    "    return kxx, dkxx\n",
    "\n",
    "def svgd(x0,p,kernel,num_iter):\n",
    "    x = x0.clone().detach()\n",
    "    N = x.shape[0]\n",
    "    D = x.shape[1]\n",
    "    x_history = torch.zeros((num_iter,N,D))\n",
    "    for it in range(num_iter):\n",
    "        x_history[it] = x.clone().detach()\n",
    "        #enable autograd for x\n",
    "        x.requires_grad = True\n",
    "        \n",
    "        #calculate kernel and kernel_gradient\n",
    "        if kernel == 'rbf':\n",
    "            kxx,dkxx = rbf_kernel(x)\n",
    "        if kernel == 'linear':\n",
    "            kxx,dkxx = linear_kernel(x)\n",
    "        #calculate the log-likelihood of x\n",
    "        logpx = p.log_prob(x)\n",
    "        #calculate the derivative of the log-likelihood wrt x\n",
    "        logpx.sum().backward(retain_graph = True)\n",
    "        dxlogpx = x.grad.clone()\n",
    "        #zero x.grad for later use\n",
    "        x.grad.data.zero_()    \n",
    "        #calculate optimal perturbation direction\n",
    "        phi = (1/N) * (torch.mm(kxx,dxlogpx) + dkxx)\n",
    "        #make a step of gradient descent\n",
    "        x_new = x + eps* phi\n",
    "        x = x_new.clone().detach()\n",
    "    return x,x_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "target_mean = torch.full([D],3,dtype=torch.float)\n",
    "#target_mean = torch.Tensor([3.,1.])\n",
    "target_covariance = torch.eye(D)*2\n",
    "#target distribution p(x)\n",
    "p = torch.distributions.MultivariateNormal(target_mean,target_covariance)\n",
    "#initial distribution q0(x)\n",
    "q0 = torch.distributions.MultivariateNormal(torch.zeros(D),torch.eye(D))\n",
    "#number of particles to sample from q0\n",
    "N = 3\n",
    "#number of iterations\n",
    "num_iter = 1000\n",
    "#step size\n",
    "eps = 1e-2\n",
    "kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize N particles from q0\n",
    "x0 = q0.sample([N])\n",
    "#copy them to the variable I want to work with (probably unneccessary)\n",
    "x = x0.clone().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_kxx = torch.zeros([N,N])\n",
    "ext_dkxx = torch.zeros([N,N,D])\n",
    "ext_dlogpx = torch.zeros([N,N,D])\n",
    "\n",
    "\n",
    "\n",
    "for it in range(num_iter):\n",
    "    #enable autograd for x\n",
    "    x.requires_grad = True\n",
    "    \n",
    "    if kernel == 'rbf':\n",
    "        dist_mat = squared_distance(x)\n",
    "        h = torch.median(dist_mat)/torch.log(torch.FloatTensor([N+1]))\n",
    "        \n",
    "    x_new = torch.zeros((N,D))\n",
    "    #for each particle determine update individually\n",
    "    for i in range(N):\n",
    "        phi = torch.zeros([1,D])\n",
    "        #determine contribution of each other particle individually\n",
    "        for j in range(N):\n",
    "            #calculate the log-likelihood of xj\n",
    "            logpx = p.log_prob(x[j])\n",
    "            #calculate the derivative of the log-likelihood wrt xj\n",
    "            logpx.sum().backward(retain_graph = True)\n",
    "            dxlogpx = x.grad[j].clone()\n",
    "            ext_dlogpx[i,j] = dxlogpx\n",
    "            #zero x.grad for later use\n",
    "            x.grad.data.zero_()\n",
    "            \n",
    "            if kernel == 'linear':\n",
    "                #determine (linear) kernel contribution\n",
    "                kxx = x[i].view(1,-1) @ x[j].view(-1,1) + 1\n",
    "                #determine linear kernel derivative wrt xj\n",
    "                \n",
    "                kxx.backward(retain_graph = True)\n",
    "                dkxx = x.grad[j].clone()\n",
    "\n",
    "                ext_kxx[i,j] = kxx\n",
    "                ext_dkxx[i,j] = dkxx\n",
    "                \n",
    "                x.grad.data.zero_()\n",
    "                    \n",
    "            elif kernel == 'rbf':\n",
    "                #determine (rbf) kernel contribution\n",
    "                kxx = torch.exp((-1/h) * dist_mat[j,i])\n",
    "                #determine rbf kernel derivative\n",
    "                dkxx = (-2/h) * kxx * (x[j] - x[i])\n",
    "                \n",
    "                ext_kxx[j,i] = kxx\n",
    "                ext_dkxx[j,i] = dkxx\n",
    "            #determine optimal perturbation direction wrt xj\n",
    "\n",
    "            phi += kxx*dxlogpx + dkxx #_auto\n",
    "        phi *= (1/N)\n",
    "        x_new[i] = x[i] + eps* phi\n",
    "    x = x_new.clone().detach()\n",
    "\n",
    "theta = x.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_svgd,_ = svgd(x0,p,kernel,num_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.8717, 2.8716])\n",
      "tensor([[3.0510, 0.3843],\n",
      "        [0.3843, 3.0510]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.mean(x_svgd,0))\n",
    "print(estimate_cov(x_svgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.8423,  0.1212],\n",
      "        [ 0.8713, -0.1863],\n",
      "        [-0.7568,  1.0121]])\n"
     ]
    }
   ],
   "source": [
    "print(x0)\n",
    "x_test = x0.clone().detach()\n",
    "x_test.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logpx_test = p.log_prob(x_test)\n",
    "logpx_test.sum().backward(retain_graph = True)\n",
    "dxlogpx_test = x_test.grad.clone()\n",
    "x_test.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "kxx_test = torch.mm(x_test,x_test.T)+torch.ones([N,N])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kxx_test.allclose(ext_kxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkxx_test = torch.zeros([N,D])\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        kxx_test[i,j].backward(retain_graph = True)\n",
    "        dkxx_test[i] += x_test.grad[j].clone()\n",
    "        x_test.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dkxx_test = torch.zeros([N,D])\n",
    "for i in range(N):\n",
    "    for j in range(N):\n",
    "        if i == j:\n",
    "            dkxx_test[i] += 2*x[i]\n",
    "        else:\n",
    "            dkxx_test[i] += x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.4564,  0.1616],\n",
       "        [ 1.1618, -0.2484],\n",
       "        [-1.0091,  1.3495]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_dkxx.mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new_test = x_test + eps * (1/N)*(kxx_test@dxlogpx_test + dkxx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new_test.allclose(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dxlogpx_test.allclose(ext_dlogpx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ext_kxx.allclose(kxx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting familiar with probability distributions in torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = 2\n",
    "m = 0\n",
    "mean = torch.ones(D) * m\n",
    "covariance = torch.eye(D)\n",
    "p = torch.distributions.MultivariateNormal(mean, covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "samples = p.sample(torch.Size([N]))\n",
    "x,y = samples[:,0],samples[:,1]\n",
    "plt.scatter(x,y)\n",
    "plt.axis('square')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
